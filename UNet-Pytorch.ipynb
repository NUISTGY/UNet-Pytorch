{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts of the U-Net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels // 2, in_channels // 2, kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = torch.tensor([x2.size()[2] - x1.size()[2]])\n",
    "        diffX = torch.tensor([x2.size()[3] - x1.size()[3]])\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full assembly of the parts to form the complete network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet(\n",
      "  (inc): DoubleConv(\n",
      "    (double_conv): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (down1): Down(\n",
      "    (maxpool_conv): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): DoubleConv(\n",
      "        (double_conv): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (down2): Down(\n",
      "    (maxpool_conv): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): DoubleConv(\n",
      "        (double_conv): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (down3): Down(\n",
      "    (maxpool_conv): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): DoubleConv(\n",
      "        (double_conv): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (down4): Down(\n",
      "    (maxpool_conv): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): DoubleConv(\n",
      "        (double_conv): Sequential(\n",
      "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (up1): Up(\n",
      "    (up): Upsample(scale_factor=2.0, mode=bilinear)\n",
      "    (conv): DoubleConv(\n",
      "      (double_conv): Sequential(\n",
      "        (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (up2): Up(\n",
      "    (up): Upsample(scale_factor=2.0, mode=bilinear)\n",
      "    (conv): DoubleConv(\n",
      "      (double_conv): Sequential(\n",
      "        (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (up3): Up(\n",
      "    (up): Upsample(scale_factor=2.0, mode=bilinear)\n",
      "    (conv): DoubleConv(\n",
      "      (double_conv): Sequential(\n",
      "        (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (up4): Up(\n",
      "    (up): Upsample(scale_factor=2.0, mode=bilinear)\n",
      "    (conv): DoubleConv(\n",
      "      (double_conv): Sequential(\n",
      "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (outc): OutConv(\n",
      "    (conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 512)\n",
    "        self.up1 = Up(1024, 256, bilinear)\n",
    "        self.up2 = Up(512, 128, bilinear)\n",
    "        self.up3 = Up(256, 64, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    net = UNet(n_channels=3, n_classes=1)\n",
    "    print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geyu\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:58: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "C:\\Users\\geyu\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:59: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss/train 0.7277677655220032\n",
      "Loss/train 0.6595970392227173\n",
      "Loss/train 0.6281815767288208\n",
      "Loss/train 0.6094270348548889\n",
      "Loss/train 0.5284303426742554\n",
      "Loss/train 0.5453810691833496\n",
      "Loss/train 0.5190775990486145\n",
      "Loss/train 0.48609039187431335\n",
      "Loss/train 0.4894610643386841\n",
      "Loss/train 0.46415722370147705\n",
      "Loss/train 0.4443259835243225\n",
      "Loss/train 0.4565272331237793\n",
      "Loss/train 0.45964720845222473\n",
      "Loss/train 0.44525468349456787\n",
      "Loss/train 0.4157237410545349\n",
      "Loss/train 0.40177133679389954\n",
      "Loss/train 0.40595850348472595\n",
      "Loss/train 0.3797835409641266\n",
      "Loss/train 0.4148378372192383\n",
      "Loss/train 0.3880782127380371\n",
      "Loss/train 0.4008256494998932\n",
      "Loss/train 0.4380495548248291\n",
      "Loss/train 0.41005077958106995\n",
      "Loss/train 0.3782820701599121\n",
      "Loss/train 0.4108103811740875\n",
      "Loss/train 0.3880535066127777\n",
      "Loss/train 0.4061937928199768\n",
      "Loss/train 0.36766961216926575\n",
      "Loss/train 0.38242197036743164\n",
      "Loss/train 0.3674065172672272\n",
      "Loss/train 0.36919093132019043\n",
      "Loss/train 0.3800937831401825\n",
      "Loss/train 0.3411778211593628\n",
      "Loss/train 0.3848046660423279\n",
      "Loss/train 0.34220603108406067\n",
      "Loss/train 0.3541482090950012\n",
      "Loss/train 0.358815461397171\n",
      "Loss/train 0.35603219270706177\n",
      "Loss/train 0.36183595657348633\n",
      "Loss/train 0.34980902075767517\n",
      "Loss/train 0.3453062176704407\n",
      "Loss/train 0.33936330676078796\n",
      "Loss/train 0.34601208567619324\n",
      "Loss/train 0.3441923260688782\n",
      "Loss/train 0.36836791038513184\n",
      "Loss/train 0.35526782274246216\n",
      "Loss/train 0.3337978422641754\n",
      "Loss/train 0.3480088710784912\n",
      "Loss/train 0.34930717945098877\n",
      "Loss/train 0.3384774923324585\n",
      "Loss/train 0.32885706424713135\n",
      "Loss/train 0.3438577353954315\n",
      "Loss/train 0.3130013942718506\n",
      "Loss/train 0.33211177587509155\n",
      "Loss/train 0.3239663243293762\n",
      "Loss/train 0.36671656370162964\n",
      "Loss/train 0.32676443457603455\n",
      "Loss/train 0.3514886498451233\n",
      "Loss/train 0.33306747674942017\n",
      "Loss/train 0.3200973868370056\n",
      "Loss/train 0.32223576307296753\n",
      "Loss/train 0.3184141516685486\n",
      "Loss/train 0.31983286142349243\n",
      "Loss/train 0.3276563584804535\n",
      "Loss/train 0.2997848391532898\n",
      "Loss/train 0.31907445192337036\n",
      "Loss/train 0.31112170219421387\n",
      "Loss/train 0.3183443546295166\n",
      "Loss/train 0.32573336362838745\n",
      "Loss/train 0.3131120800971985\n",
      "Loss/train 0.31850549578666687\n",
      "Loss/train 0.317025363445282\n",
      "Loss/train 0.32227665185928345\n",
      "Loss/train 0.3309825658798218\n",
      "Loss/train 0.34267961978912354\n",
      "Loss/train 0.3635292649269104\n",
      "Loss/train 0.310369074344635\n",
      "Loss/train 0.3203396499156952\n",
      "Loss/train 0.31399065256118774\n",
      "Loss/train 0.3230236768722534\n",
      "Loss/train 0.2961064577102661\n",
      "Loss/train 0.30702564120292664\n",
      "Loss/train 0.3141147792339325\n",
      "Loss/train 0.3034684360027313\n",
      "Loss/train 0.29690688848495483\n",
      "Loss/train 0.31787508726119995\n",
      "Loss/train 0.30083972215652466\n",
      "Loss/train 0.30039042234420776\n",
      "Loss/train 0.3115648031234741\n",
      "Loss/train 0.31526440382003784\n",
      "Loss/train 0.3076881170272827\n",
      "Loss/train 0.312799870967865\n",
      "Loss/train 0.2821476459503174\n",
      "Loss/train 0.30595874786376953\n",
      "Loss/train 0.31653183698654175\n",
      "Loss/train 0.3031861186027527\n",
      "Loss/train 0.2852747440338135\n",
      "Loss/train 0.29425039887428284\n",
      "Loss/train 0.3137314021587372\n",
      "Loss/train 0.29646649956703186\n",
      "Loss/train 0.29159218072891235\n",
      "Loss/train 0.29185736179351807\n",
      "Loss/train 0.2945210933685303\n",
      "Loss/train 0.3219811022281647\n",
      "Loss/train 0.2954025864601135\n",
      "Loss/train 0.28958407044410706\n",
      "Loss/train 0.33073902130126953\n",
      "Loss/train 0.2923659086227417\n",
      "Loss/train 0.28573042154312134\n",
      "Loss/train 0.3149573802947998\n",
      "Loss/train 0.31488028168678284\n",
      "Loss/train 0.29508137702941895\n",
      "Loss/train 0.3058757185935974\n",
      "Loss/train 0.31780457496643066\n",
      "Loss/train 0.31027260422706604\n",
      "Loss/train 0.2943560481071472\n",
      "Loss/train 0.28825318813323975\n",
      "Loss/train 0.2988871932029724\n",
      "Loss/train 0.2810463607311249\n",
      "Loss/train 0.2909274399280548\n",
      "Loss/train 0.30006009340286255\n",
      "Loss/train 0.27732616662979126\n",
      "Loss/train 0.30470848083496094\n",
      "Loss/train 0.2917386293411255\n",
      "Loss/train 0.330341637134552\n",
      "Loss/train 0.29123833775520325\n",
      "Loss/train 0.2880409061908722\n",
      "Loss/train 0.2863118648529053\n",
      "Loss/train 0.2831077575683594\n",
      "Loss/train 0.28293704986572266\n",
      "Loss/train 0.2806605398654938\n",
      "Loss/train 0.29229074716567993\n",
      "Loss/train 0.2694738507270813\n",
      "Loss/train 0.28683924674987793\n",
      "Loss/train 0.2787666916847229\n",
      "Loss/train 0.28022894263267517\n",
      "Loss/train 0.2807135581970215\n",
      "Loss/train 0.27532893419265747\n",
      "Loss/train 0.2845997214317322\n",
      "Loss/train 0.30397653579711914\n",
      "Loss/train 0.2850611209869385\n",
      "Loss/train 0.27444183826446533\n",
      "Loss/train 0.2851877212524414\n",
      "Loss/train 0.27514398097991943\n",
      "Loss/train 0.288261353969574\n",
      "Loss/train 0.30230948328971863\n",
      "Loss/train 0.2928253412246704\n",
      "Loss/train 0.2791348695755005\n",
      "Loss/train 0.29809316992759705\n",
      "Loss/train 0.2669403553009033\n",
      "Loss/train 0.2792097330093384\n",
      "Loss/train 0.2830154597759247\n",
      "Loss/train 0.28285104036331177\n",
      "Loss/train 0.2758079767227173\n",
      "Loss/train 0.2698283791542053\n",
      "Loss/train 0.28107231855392456\n",
      "Loss/train 0.2735023498535156\n",
      "Loss/train 0.2641141712665558\n",
      "Loss/train 0.27616915106773376\n",
      "Loss/train 0.2789705693721771\n",
      "Loss/train 0.2581750452518463\n",
      "Loss/train 0.26516786217689514\n",
      "Loss/train 0.28494036197662354\n",
      "Loss/train 0.26383176445961\n",
      "Loss/train 0.27751588821411133\n",
      "Loss/train 0.3206080198287964\n",
      "Loss/train 0.27339863777160645\n",
      "Loss/train 0.2706524729728699\n",
      "Loss/train 0.27328425645828247\n",
      "Loss/train 0.2585933208465576\n",
      "Loss/train 0.25773122906684875\n",
      "Loss/train 0.2779862582683563\n",
      "Loss/train 0.2707189917564392\n",
      "Loss/train 0.27290740609169006\n",
      "Loss/train 0.2514752447605133\n",
      "Loss/train 0.2587011754512787\n",
      "Loss/train 0.26538586616516113\n",
      "Loss/train 0.2870303988456726\n",
      "Loss/train 0.26003769040107727\n",
      "Loss/train 0.28804612159729004\n",
      "Loss/train 0.2549647390842438\n",
      "Loss/train 0.26942142844200134\n",
      "Loss/train 0.28983402252197266\n",
      "Loss/train 0.2736671566963196\n",
      "Loss/train 0.2788938283920288\n",
      "Loss/train 0.26044660806655884\n",
      "Loss/train 0.2740726172924042\n",
      "Loss/train 0.27335643768310547\n",
      "Loss/train 0.2958814799785614\n",
      "Loss/train 0.2979525327682495\n",
      "Loss/train 0.26547712087631226\n",
      "Loss/train 0.26081186532974243\n",
      "Loss/train 0.2552534341812134\n",
      "Loss/train 0.26535481214523315\n",
      "Loss/train 0.24671916663646698\n",
      "Loss/train 0.2651534676551819\n",
      "Loss/train 0.2720475196838379\n",
      "Loss/train 0.2624601125717163\n",
      "Loss/train 0.26987701654434204\n",
      "Loss/train 0.2595406472682953\n",
      "Loss/train 0.26627835631370544\n",
      "Loss/train 0.2530670762062073\n",
      "Loss/train 0.2552618384361267\n",
      "Loss/train 0.27520036697387695\n",
      "Loss/train 0.2607835531234741\n",
      "Loss/train 0.27583059668540955\n",
      "Loss/train 0.24722041189670563\n",
      "Loss/train 0.26500654220581055\n",
      "Loss/train 0.2503167390823364\n",
      "Loss/train 0.2707185447216034\n",
      "Loss/train 0.2535466253757477\n",
      "Loss/train 0.28092101216316223\n",
      "Loss/train 0.2576044797897339\n",
      "Loss/train 0.2607704699039459\n",
      "Loss/train 0.2626666724681854\n",
      "Loss/train 0.2618754804134369\n",
      "Loss/train 0.2749235928058624\n",
      "Loss/train 0.23954223096370697\n",
      "Loss/train 0.25860151648521423\n",
      "Loss/train 0.24341526627540588\n",
      "Loss/train 0.24659940600395203\n",
      "Loss/train 0.24220363795757294\n",
      "Loss/train 0.29200953245162964\n",
      "Loss/train 0.26032817363739014\n",
      "Loss/train 0.25736138224601746\n",
      "Loss/train 0.248454749584198\n",
      "Loss/train 0.2660422623157501\n",
      "Loss/train 0.25166767835617065\n",
      "Loss/train 0.24377913773059845\n",
      "Loss/train 0.25003838539123535\n",
      "Loss/train 0.24919387698173523\n",
      "Loss/train 0.2565586566925049\n",
      "Loss/train 0.25075483322143555\n",
      "Loss/train 0.2437286376953125\n",
      "Loss/train 0.2319682538509369\n",
      "Loss/train 0.2637895941734314\n",
      "Loss/train 0.25363466143608093\n",
      "Loss/train 0.24177049100399017\n",
      "Loss/train 0.2638530731201172\n",
      "Loss/train 0.24092629551887512\n",
      "Loss/train 0.24422770738601685\n",
      "Loss/train 0.2390584647655487\n",
      "Loss/train 0.2794908285140991\n",
      "Loss/train 0.23477265238761902\n",
      "Loss/train 0.2385648787021637\n",
      "Loss/train 0.24393099546432495\n",
      "Loss/train 0.2336014062166214\n",
      "Loss/train 0.26044172048568726\n",
      "Loss/train 0.2529747486114502\n",
      "Loss/train 0.25502705574035645\n",
      "Loss/train 0.23489366471767426\n",
      "Loss/train 0.25036561489105225\n",
      "Loss/train 0.2443746030330658\n",
      "Loss/train 0.24575653672218323\n",
      "Loss/train 0.2321808934211731\n",
      "Loss/train 0.2502875328063965\n",
      "Loss/train 0.23915350437164307\n",
      "Loss/train 0.25365662574768066\n",
      "Loss/train 0.24117374420166016\n",
      "Loss/train 0.23938718438148499\n",
      "Loss/train 0.2378183752298355\n",
      "Loss/train 0.23162537813186646\n",
      "Loss/train 0.24614201486110687\n",
      "Loss/train 0.2828587293624878\n",
      "Loss/train 0.25081101059913635\n",
      "Loss/train 0.24925746023654938\n",
      "Loss/train 0.2511124014854431\n",
      "Loss/train 0.27672189474105835\n",
      "Loss/train 0.25727760791778564\n",
      "Loss/train 0.23486778140068054\n",
      "Loss/train 0.26272183656692505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss/train 0.25738129019737244\n",
      "Loss/train 0.24069589376449585\n",
      "Loss/train 0.2636692523956299\n",
      "Loss/train 0.2586187720298767\n",
      "Loss/train 0.2349618822336197\n",
      "Loss/train 0.2571025490760803\n",
      "Loss/train 0.2513042390346527\n",
      "Loss/train 0.2521103024482727\n",
      "Loss/train 0.27424895763397217\n",
      "Loss/train 0.2535446882247925\n",
      "Loss/train 0.24738043546676636\n",
      "Loss/train 0.2270861566066742\n",
      "Loss/train 0.2721731662750244\n",
      "Loss/train 0.2570911645889282\n",
      "Loss/train 0.2501601278781891\n",
      "Loss/train 0.23039492964744568\n",
      "Loss/train 0.22945666313171387\n",
      "Loss/train 0.233295738697052\n",
      "Loss/train 0.22871826589107513\n",
      "Loss/train 0.2348945587873459\n",
      "Loss/train 0.23606997728347778\n",
      "Loss/train 0.21930333971977234\n",
      "Loss/train 0.23055672645568848\n",
      "Loss/train 0.2365962564945221\n",
      "Loss/train 0.22632503509521484\n",
      "Loss/train 0.24971629679203033\n",
      "Loss/train 0.24809543788433075\n",
      "Loss/train 0.2326025366783142\n",
      "Loss/train 0.22084076702594757\n",
      "Loss/train 0.22340573370456696\n",
      "Loss/train 0.2293793261051178\n",
      "Loss/train 0.22727075219154358\n",
      "Loss/train 0.2654684782028198\n",
      "Loss/train 0.2255108654499054\n",
      "Loss/train 0.21507349610328674\n",
      "Loss/train 0.23648233711719513\n",
      "Loss/train 0.23964092135429382\n",
      "Loss/train 0.23987703025341034\n",
      "Loss/train 0.2234121710062027\n",
      "Loss/train 0.25509631633758545\n",
      "Loss/train 0.22311589121818542\n",
      "Loss/train 0.22904324531555176\n",
      "Loss/train 0.2309017926454544\n",
      "Loss/train 0.24328085780143738\n",
      "Loss/train 0.2300623655319214\n",
      "Loss/train 0.23180848360061646\n",
      "Loss/train 0.25686806440353394\n",
      "Loss/train 0.23670543730258942\n",
      "Loss/train 0.22854916751384735\n",
      "Loss/train 0.23435631394386292\n",
      "Loss/train 0.21847783029079437\n",
      "Loss/train 0.22839893400669098\n",
      "Loss/train 0.23278473317623138\n",
      "Loss/train 0.22988256812095642\n",
      "Loss/train 0.23037093877792358\n",
      "Loss/train 0.24342316389083862\n",
      "Loss/train 0.214875727891922\n",
      "Loss/train 0.22777505218982697\n",
      "Loss/train 0.2279348522424698\n",
      "Loss/train 0.22922761738300323\n",
      "Loss/train 0.21354830265045166\n",
      "Loss/train 0.2586289048194885\n",
      "Loss/train 0.21850694715976715\n",
      "Loss/train 0.22068750858306885\n",
      "Loss/train 0.2200665920972824\n",
      "Loss/train 0.21309828758239746\n",
      "Loss/train 0.23114541172981262\n",
      "Loss/train 0.209090918302536\n",
      "Loss/train 0.22533218562602997\n",
      "Loss/train 0.2149651050567627\n",
      "Loss/train 0.2392212450504303\n",
      "Loss/train 0.24635444581508636\n",
      "Loss/train 0.21829235553741455\n",
      "Loss/train 0.23469410836696625\n",
      "Loss/train 0.20394757390022278\n",
      "Loss/train 0.21965086460113525\n",
      "Loss/train 0.22356650233268738\n",
      "Loss/train 0.21349871158599854\n",
      "Loss/train 0.21575307846069336\n",
      "Loss/train 0.22281499207019806\n",
      "Loss/train 0.2189830243587494\n",
      "Loss/train 0.21117153763771057\n",
      "Loss/train 0.21622300148010254\n",
      "Loss/train 0.21852952241897583\n",
      "Loss/train 0.2652609944343567\n",
      "Loss/train 0.21351006627082825\n",
      "Loss/train 0.2069198489189148\n",
      "Loss/train 0.2308780997991562\n",
      "Loss/train 0.2275269478559494\n",
      "Loss/train 0.20838947594165802\n",
      "Loss/train 0.2321922481060028\n",
      "Loss/train 0.2114216387271881\n",
      "Loss/train 0.20859205722808838\n",
      "Loss/train 0.2128787636756897\n",
      "Loss/train 0.21496909856796265\n",
      "Loss/train 0.20634588599205017\n",
      "Loss/train 0.21329055726528168\n",
      "Loss/train 0.2025350034236908\n",
      "Loss/train 0.21683794260025024\n",
      "Loss/train 0.2289937436580658\n",
      "Loss/train 0.21037326753139496\n",
      "Loss/train 0.2117372751235962\n",
      "Loss/train 0.19690755009651184\n",
      "Loss/train 0.2259744107723236\n",
      "Loss/train 0.20786115527153015\n",
      "Loss/train 0.20991279184818268\n",
      "Loss/train 0.21242524683475494\n",
      "Loss/train 0.22499842941761017\n",
      "Loss/train 0.21347640454769135\n",
      "Loss/train 0.19914400577545166\n",
      "Loss/train 0.20566408336162567\n",
      "Loss/train 0.2171286940574646\n",
      "Loss/train 0.20605501532554626\n",
      "Loss/train 0.2741560935974121\n",
      "Loss/train 0.20290330052375793\n",
      "Loss/train 0.2100764364004135\n",
      "Loss/train 0.24486812949180603\n",
      "Loss/train 0.22012212872505188\n",
      "Loss/train 0.23093350231647491\n",
      "Loss/train 0.24132858216762543\n",
      "Loss/train 0.2156251221895218\n",
      "Loss/train 0.22436173260211945\n",
      "Loss/train 0.22021913528442383\n",
      "Loss/train 0.22016045451164246\n",
      "Loss/train 0.20603471994400024\n",
      "Loss/train 0.20031681656837463\n",
      "Loss/train 0.22858062386512756\n",
      "Loss/train 0.19193018972873688\n",
      "Loss/train 0.2033846080303192\n",
      "Loss/train 0.21901604533195496\n",
      "Loss/train 0.23153766989707947\n",
      "Loss/train 0.21662992238998413\n",
      "Loss/train 0.20278817415237427\n",
      "Loss/train 0.20574787259101868\n",
      "Loss/train 0.20386026799678802\n",
      "Loss/train 0.20818158984184265\n",
      "Loss/train 0.19659306108951569\n",
      "Loss/train 0.21271415054798126\n",
      "Loss/train 0.20915880799293518\n",
      "Loss/train 0.19145402312278748\n",
      "Loss/train 0.2056952863931656\n",
      "Loss/train 0.19289126992225647\n",
      "Loss/train 0.19982478022575378\n",
      "Loss/train 0.21500077843666077\n",
      "Loss/train 0.22302575409412384\n",
      "Loss/train 0.21041736006736755\n",
      "Loss/train 0.22523295879364014\n",
      "Loss/train 0.2293207049369812\n",
      "Loss/train 0.2000381499528885\n",
      "Loss/train 0.19840657711029053\n",
      "Loss/train 0.20265692472457886\n",
      "Loss/train 0.20792391896247864\n",
      "Loss/train 0.20763260126113892\n",
      "Loss/train 0.20965197682380676\n",
      "Loss/train 0.20377635955810547\n",
      "Loss/train 0.2254360169172287\n",
      "Loss/train 0.23372307419776917\n",
      "Loss/train 0.2002888172864914\n",
      "Loss/train 0.20168772339820862\n",
      "Loss/train 0.20470139384269714\n",
      "Loss/train 0.1980878710746765\n",
      "Loss/train 0.21204859018325806\n",
      "Loss/train 0.20150817930698395\n",
      "Loss/train 0.20397447049617767\n",
      "Loss/train 0.21459829807281494\n",
      "Loss/train 0.19044113159179688\n",
      "Loss/train 0.20054298639297485\n",
      "Loss/train 0.19214142858982086\n",
      "Loss/train 0.21846820414066315\n",
      "Loss/train 0.20322778820991516\n",
      "Loss/train 0.21914121508598328\n",
      "Loss/train 0.19820791482925415\n",
      "Loss/train 0.21908819675445557\n",
      "Loss/train 0.2489657700061798\n",
      "Loss/train 0.235066756606102\n",
      "Loss/train 0.20169983804225922\n",
      "Loss/train 0.22168219089508057\n",
      "Loss/train 0.22704795002937317\n",
      "Loss/train 0.21349449455738068\n",
      "Loss/train 0.22973589599132538\n",
      "Loss/train 0.21306250989437103\n",
      "Loss/train 0.19448059797286987\n",
      "Loss/train 0.21163725852966309\n",
      "Loss/train 0.20077478885650635\n",
      "Loss/train 0.23882977664470673\n",
      "Loss/train 0.19243387877941132\n",
      "Loss/train 0.21031171083450317\n",
      "Loss/train 0.23730488121509552\n",
      "Loss/train 0.2009182721376419\n",
      "Loss/train 0.1969933956861496\n",
      "Loss/train 0.19440853595733643\n",
      "Loss/train 0.2124847173690796\n",
      "Loss/train 0.19290147721767426\n",
      "Loss/train 0.22713126242160797\n",
      "Loss/train 0.21518947184085846\n",
      "Loss/train 0.18790870904922485\n",
      "Loss/train 0.19402214884757996\n",
      "Loss/train 0.18755561113357544\n",
      "Loss/train 0.1990508884191513\n",
      "Loss/train 0.20045205950737\n",
      "Loss/train 0.1987866461277008\n",
      "Loss/train 0.1876140683889389\n",
      "Loss/train 0.212149515748024\n",
      "Loss/train 0.19263297319412231\n",
      "Loss/train 0.195531964302063\n",
      "Loss/train 0.19816702604293823\n",
      "Loss/train 0.19683781266212463\n",
      "Loss/train 0.20134907960891724\n",
      "Loss/train 0.1910383701324463\n",
      "Loss/train 0.20235250890254974\n",
      "Loss/train 0.19598713517189026\n",
      "Loss/train 0.18800120055675507\n",
      "Loss/train 0.22268301248550415\n",
      "Loss/train 0.21528156101703644\n",
      "Loss/train 0.18666838109493256\n",
      "Loss/train 0.18628241121768951\n",
      "Loss/train 0.1899983435869217\n",
      "Loss/train 0.21545732021331787\n",
      "Loss/train 0.19797173142433167\n",
      "Loss/train 0.22038835287094116\n",
      "Loss/train 0.22713744640350342\n",
      "Loss/train 0.18292488157749176\n",
      "Loss/train 0.1971462219953537\n",
      "Loss/train 0.18449756503105164\n",
      "Loss/train 0.1998763382434845\n",
      "Loss/train 0.18610350787639618\n",
      "Loss/train 0.20460134744644165\n",
      "Loss/train 0.19854368269443512\n",
      "Loss/train 0.19770199060440063\n",
      "Loss/train 0.18361888825893402\n",
      "Loss/train 0.1955450475215912\n",
      "Loss/train 0.18852843344211578\n",
      "Loss/train 0.20083680748939514\n",
      "Loss/train 0.1920490711927414\n",
      "Loss/train 0.17616747319698334\n",
      "Loss/train 0.20544910430908203\n",
      "Loss/train 0.1875256896018982\n",
      "Loss/train 0.19805169105529785\n",
      "Loss/train 0.20026937127113342\n",
      "Loss/train 0.19408559799194336\n",
      "Loss/train 0.18818697333335876\n",
      "Loss/train 0.1819104701280594\n",
      "Loss/train 0.19236408174037933\n",
      "Loss/train 0.1769980788230896\n",
      "Loss/train 0.18577486276626587\n",
      "Loss/train 0.1856776475906372\n",
      "Loss/train 0.1998833864927292\n",
      "Loss/train 0.17132440209388733\n",
      "Loss/train 0.2105669230222702\n",
      "Loss/train 0.19546173512935638\n",
      "Loss/train 0.19918318092823029\n",
      "Loss/train 0.20748481154441833\n",
      "Loss/train 0.18308314681053162\n",
      "Loss/train 0.18072447180747986\n",
      "Loss/train 0.18611091375350952\n",
      "Loss/train 0.20694860816001892\n",
      "Loss/train 0.20313520729541779\n",
      "Loss/train 0.1885760873556137\n",
      "Loss/train 0.18913909792900085\n",
      "Loss/train 0.18389099836349487\n",
      "Loss/train 0.18821609020233154\n",
      "Loss/train 0.18520231544971466\n",
      "Loss/train 0.18561863899230957\n",
      "Loss/train 0.18344302475452423\n",
      "Loss/train 0.19969770312309265\n",
      "Loss/train 0.20639920234680176\n",
      "Loss/train 0.21234434843063354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss/train 0.2299439013004303\n",
      "Loss/train 0.22793111205101013\n",
      "Loss/train 0.17342647910118103\n",
      "Loss/train 0.18887339532375336\n",
      "Loss/train 0.2103615701198578\n",
      "Loss/train 0.18673530220985413\n",
      "Loss/train 0.18010520935058594\n",
      "Loss/train 0.18429622054100037\n",
      "Loss/train 0.19264158606529236\n",
      "Loss/train 0.17047753930091858\n",
      "Loss/train 0.18863004446029663\n",
      "Loss/train 0.18533477187156677\n",
      "Loss/train 0.2042684704065323\n",
      "Loss/train 0.21969084441661835\n",
      "Loss/train 0.1824648678302765\n",
      "Loss/train 0.18202880024909973\n",
      "Loss/train 0.19869495928287506\n",
      "Loss/train 0.16918835043907166\n",
      "Loss/train 0.16665522754192352\n",
      "Loss/train 0.17898061871528625\n",
      "Loss/train 0.16574181616306305\n",
      "Loss/train 0.17288286983966827\n",
      "Loss/train 0.19583475589752197\n",
      "Loss/train 0.19333821535110474\n",
      "Loss/train 0.1847568154335022\n",
      "Loss/train 0.1824151873588562\n",
      "Loss/train 0.18232271075248718\n",
      "Loss/train 0.1893659383058548\n",
      "Loss/train 0.17971718311309814\n",
      "Loss/train 0.18982283771038055\n",
      "Loss/train 0.17911681532859802\n",
      "Loss/train 0.17049047350883484\n",
      "Loss/train 0.17499282956123352\n",
      "Loss/train 0.17430579662322998\n",
      "Loss/train 0.2157268524169922\n",
      "Loss/train 0.1725141704082489\n",
      "Loss/train 0.18355293571949005\n",
      "Loss/train 0.18921178579330444\n",
      "Loss/train 0.18831783533096313\n",
      "Loss/train 0.1780608892440796\n",
      "Loss/train 0.16681742668151855\n",
      "Loss/train 0.18178865313529968\n",
      "Loss/train 0.18408933281898499\n",
      "Loss/train 0.184798002243042\n",
      "Loss/train 0.18456538021564484\n",
      "Loss/train 0.17509552836418152\n",
      "Loss/train 0.2108881026506424\n",
      "Loss/train 0.1789778769016266\n",
      "Loss/train 0.18789330124855042\n",
      "Loss/train 0.17725226283073425\n",
      "Loss/train 0.19029568135738373\n",
      "Loss/train 0.18657329678535461\n",
      "Loss/train 0.17860908806324005\n",
      "Loss/train 0.207147479057312\n",
      "Loss/train 0.16972078382968903\n",
      "Loss/train 0.17325332760810852\n",
      "Loss/train 0.18911144137382507\n",
      "Loss/train 0.17766176164150238\n",
      "Loss/train 0.18283864855766296\n",
      "Loss/train 0.1697329431772232\n",
      "Loss/train 0.182363823056221\n",
      "Loss/train 0.2222638577222824\n",
      "Loss/train 0.19598056375980377\n",
      "Loss/train 0.19605126976966858\n",
      "Loss/train 0.17141911387443542\n",
      "Loss/train 0.18726997077465057\n",
      "Loss/train 0.18137750029563904\n",
      "Loss/train 0.18581736087799072\n",
      "Loss/train 0.17554020881652832\n",
      "Loss/train 0.16601887345314026\n",
      "Loss/train 0.1676100492477417\n",
      "Loss/train 0.17135398089885712\n",
      "Loss/train 0.16680936515331268\n",
      "Loss/train 0.16199827194213867\n",
      "Loss/train 0.17787504196166992\n",
      "Loss/train 0.1689571887254715\n",
      "Loss/train 0.1656912863254547\n",
      "Loss/train 0.16302698850631714\n",
      "Loss/train 0.1642782837152481\n",
      "Loss/train 0.18292713165283203\n",
      "Loss/train 0.17811746895313263\n",
      "Loss/train 0.19203093647956848\n",
      "Loss/train 0.17197352647781372\n",
      "Loss/train 0.16831371188163757\n",
      "Loss/train 0.16948960721492767\n",
      "Loss/train 0.19927576184272766\n",
      "Loss/train 0.16952817142009735\n",
      "Loss/train 0.17300984263420105\n",
      "Loss/train 0.16634756326675415\n",
      "Loss/train 0.16110873222351074\n",
      "Loss/train 0.2001078575849533\n",
      "Loss/train 0.20409759879112244\n",
      "Loss/train 0.16479817032814026\n",
      "Loss/train 0.18185390532016754\n",
      "Loss/train 0.17986464500427246\n",
      "Loss/train 0.18675535917282104\n",
      "Loss/train 0.1629672646522522\n",
      "Loss/train 0.1630595326423645\n",
      "Loss/train 0.15936730802059174\n",
      "Loss/train 0.1870444416999817\n",
      "Loss/train 0.16992118954658508\n",
      "Loss/train 0.1946238875389099\n",
      "Loss/train 0.1863008737564087\n",
      "Loss/train 0.15925686061382294\n",
      "Loss/train 0.16890473663806915\n",
      "Loss/train 0.1603042632341385\n",
      "Loss/train 0.1727447211742401\n",
      "Loss/train 0.15383455157279968\n",
      "Loss/train 0.16363635659217834\n",
      "Loss/train 0.18914088606834412\n",
      "Loss/train 0.19637851417064667\n",
      "Loss/train 0.17081902921199799\n",
      "Loss/train 0.17720919847488403\n",
      "Loss/train 0.16985732316970825\n",
      "Loss/train 0.18403656780719757\n",
      "Loss/train 0.18123123049736023\n",
      "Loss/train 0.1766442507505417\n",
      "Loss/train 0.17036543786525726\n",
      "Loss/train 0.1794976145029068\n",
      "Loss/train 0.18330353498458862\n",
      "Loss/train 0.16266381740570068\n",
      "Loss/train 0.1667836606502533\n",
      "Loss/train 0.15681079030036926\n",
      "Loss/train 0.16045919060707092\n",
      "Loss/train 0.1654912233352661\n",
      "Loss/train 0.185434490442276\n",
      "Loss/train 0.1768878698348999\n",
      "Loss/train 0.17085996270179749\n",
      "Loss/train 0.16252507269382477\n",
      "Loss/train 0.16564702987670898\n",
      "Loss/train 0.15720731019973755\n",
      "Loss/train 0.16013303399085999\n",
      "Loss/train 0.15195894241333008\n",
      "Loss/train 0.1642770767211914\n",
      "Loss/train 0.16207456588745117\n",
      "Loss/train 0.20262698829174042\n",
      "Loss/train 0.16125506162643433\n",
      "Loss/train 0.1550089716911316\n",
      "Loss/train 0.1622544229030609\n",
      "Loss/train 0.1892341524362564\n",
      "Loss/train 0.15478384494781494\n",
      "Loss/train 0.16590183973312378\n",
      "Loss/train 0.18024635314941406\n",
      "Loss/train 0.18751396238803864\n",
      "Loss/train 0.19229841232299805\n",
      "Loss/train 0.15739434957504272\n",
      "Loss/train 0.17371532320976257\n",
      "Loss/train 0.17588768899440765\n",
      "Loss/train 0.17832306027412415\n",
      "Loss/train 0.16554537415504456\n",
      "Loss/train 0.17428548634052277\n",
      "Loss/train 0.17341169714927673\n",
      "Loss/train 0.15947073698043823\n",
      "Loss/train 0.15801337361335754\n",
      "Loss/train 0.1699429452419281\n",
      "Loss/train 0.15528175234794617\n",
      "Loss/train 0.16602513194084167\n",
      "Loss/train 0.15680837631225586\n",
      "Loss/train 0.17657780647277832\n",
      "Loss/train 0.1688852161169052\n",
      "Loss/train 0.1488223820924759\n",
      "Loss/train 0.18045231699943542\n",
      "Loss/train 0.1548817753791809\n",
      "Loss/train 0.15839369595050812\n",
      "Loss/train 0.19320203363895416\n",
      "Loss/train 0.1674118936061859\n",
      "Loss/train 0.17662101984024048\n",
      "Loss/train 0.15598474442958832\n",
      "Loss/train 0.14691847562789917\n",
      "Loss/train 0.20103532075881958\n",
      "Loss/train 0.15298894047737122\n",
      "Loss/train 0.16760152578353882\n",
      "Loss/train 0.15212811529636383\n",
      "Loss/train 0.1675194352865219\n",
      "Loss/train 0.18329674005508423\n",
      "Loss/train 0.17858412861824036\n",
      "Loss/train 0.16025160253047943\n",
      "Loss/train 0.17379814386367798\n",
      "Loss/train 0.15910831093788147\n",
      "Loss/train 0.15822948515415192\n",
      "Loss/train 0.16922128200531006\n",
      "Loss/train 0.15132707357406616\n",
      "Loss/train 0.1571534276008606\n",
      "Loss/train 0.156368225812912\n",
      "Loss/train 0.16907896101474762\n",
      "Loss/train 0.17268136143684387\n",
      "Loss/train 0.16548612713813782\n",
      "Loss/train 0.17000721395015717\n",
      "Loss/train 0.14925912022590637\n",
      "Loss/train 0.15396487712860107\n",
      "Loss/train 0.15456342697143555\n",
      "Loss/train 0.16155166923999786\n",
      "Loss/train 0.14850769937038422\n",
      "Loss/train 0.1553412675857544\n",
      "Loss/train 0.18005934357643127\n",
      "Loss/train 0.14606212079524994\n",
      "Loss/train 0.18771858513355255\n",
      "Loss/train 0.1902991533279419\n",
      "Loss/train 0.16126668453216553\n",
      "Loss/train 0.15116599202156067\n",
      "Loss/train 0.15672971308231354\n",
      "Loss/train 0.16360124945640564\n",
      "Loss/train 0.14660753309726715\n",
      "Loss/train 0.17308712005615234\n",
      "Loss/train 0.1435125321149826\n",
      "Loss/train 0.14693474769592285\n",
      "Loss/train 0.15622521936893463\n",
      "Loss/train 0.15947766602039337\n",
      "Loss/train 0.1717698574066162\n",
      "Loss/train 0.15646210312843323\n",
      "Loss/train 0.1560789942741394\n",
      "Loss/train 0.16684260964393616\n",
      "Loss/train 0.16896149516105652\n",
      "Loss/train 0.1402251422405243\n",
      "Loss/train 0.14477069675922394\n",
      "Loss/train 0.15269431471824646\n",
      "Loss/train 0.14691615104675293\n",
      "Loss/train 0.15561987459659576\n",
      "Loss/train 0.15466836094856262\n",
      "Loss/train 0.15235823392868042\n",
      "Loss/train 0.15643610060214996\n",
      "Loss/train 0.14071834087371826\n",
      "Loss/train 0.17891038954257965\n",
      "Loss/train 0.1721315234899521\n",
      "Loss/train 0.16166232526302338\n",
      "Loss/train 0.1765454113483429\n",
      "Loss/train 0.16040557622909546\n",
      "Loss/train 0.15817385911941528\n",
      "Loss/train 0.1623302400112152\n",
      "Loss/train 0.18825803697109222\n",
      "Loss/train 0.15881440043449402\n",
      "Loss/train 0.1532275378704071\n",
      "Loss/train 0.18022099137306213\n",
      "Loss/train 0.16299939155578613\n",
      "Loss/train 0.15690672397613525\n",
      "Loss/train 0.17102353274822235\n",
      "Loss/train 0.14756977558135986\n",
      "Loss/train 0.16256025433540344\n",
      "Loss/train 0.17344185709953308\n",
      "Loss/train 0.1636766791343689\n",
      "Loss/train 0.15934529900550842\n",
      "Loss/train 0.1716313660144806\n",
      "Loss/train 0.15410278737545013\n",
      "Loss/train 0.1493425965309143\n",
      "Loss/train 0.17580437660217285\n",
      "Loss/train 0.14347493648529053\n",
      "Loss/train 0.14213037490844727\n",
      "Loss/train 0.15420711040496826\n",
      "Loss/train 0.14114351570606232\n",
      "Loss/train 0.14505666494369507\n",
      "Loss/train 0.14420419931411743\n",
      "Loss/train 0.16105595231056213\n",
      "Loss/train 0.1383211612701416\n",
      "Loss/train 0.16260817646980286\n",
      "Loss/train 0.1385108232498169\n",
      "Loss/train 0.14080312848091125\n",
      "Loss/train 0.14053133130073547\n",
      "Loss/train 0.14354026317596436\n",
      "Loss/train 0.14968138933181763\n",
      "Loss/train 0.158610999584198\n",
      "Loss/train 0.17051821947097778\n",
      "Loss/train 0.16388389468193054\n",
      "Loss/train 0.1559520959854126\n",
      "Loss/train 0.1545356661081314\n",
      "Loss/train 0.16564039885997772\n",
      "Loss/train 0.15984024107456207\n",
      "Loss/train 0.14645156264305115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss/train 0.17719686031341553\n",
      "Loss/train 0.1413010060787201\n",
      "Loss/train 0.17092236876487732\n",
      "Loss/train 0.16432228684425354\n",
      "Loss/train 0.15279464423656464\n",
      "Loss/train 0.14312590658664703\n",
      "Loss/train 0.13402500748634338\n",
      "Loss/train 0.15148313343524933\n",
      "Loss/train 0.15432003140449524\n",
      "Loss/train 0.144993394613266\n",
      "Loss/train 0.14424720406532288\n",
      "Loss/train 0.15283477306365967\n",
      "Loss/train 0.15073351562023163\n",
      "Loss/train 0.15446437895298004\n",
      "Loss/train 0.14013884961605072\n",
      "Loss/train 0.1355498731136322\n",
      "Loss/train 0.1649799793958664\n",
      "Loss/train 0.1529901623725891\n",
      "Loss/train 0.1470465362071991\n",
      "Loss/train 0.13479724526405334\n",
      "Loss/train 0.16566714644432068\n",
      "Loss/train 0.1576082408428192\n",
      "Loss/train 0.15658248960971832\n",
      "Loss/train 0.14538197219371796\n",
      "Loss/train 0.14157456159591675\n",
      "Loss/train 0.16477283835411072\n",
      "Loss/train 0.18290257453918457\n",
      "Loss/train 0.14895862340927124\n",
      "Loss/train 0.133034348487854\n",
      "Loss/train 0.14616909623146057\n",
      "Loss/train 0.14337316155433655\n",
      "Loss/train 0.13867905735969543\n",
      "Loss/train 0.15183520317077637\n",
      "Loss/train 0.1523134857416153\n",
      "Loss/train 0.14914456009864807\n",
      "Loss/train 0.17385414242744446\n",
      "Loss/train 0.1404794305562973\n",
      "Loss/train 0.15764309465885162\n",
      "Loss/train 0.15224874019622803\n",
      "Loss/train 0.15942612290382385\n",
      "Loss/train 0.13259775936603546\n",
      "Loss/train 0.14739936590194702\n",
      "Loss/train 0.14085513353347778\n",
      "Loss/train 0.13558921217918396\n",
      "Loss/train 0.1297881156206131\n",
      "Loss/train 0.14985978603363037\n",
      "Loss/train 0.15300492942333221\n",
      "Loss/train 0.13583633303642273\n",
      "Loss/train 0.14083480834960938\n",
      "Loss/train 0.13668350875377655\n",
      "Loss/train 0.1311548352241516\n",
      "Loss/train 0.1362011730670929\n",
      "Loss/train 0.14294381439685822\n",
      "Loss/train 0.14305445551872253\n",
      "Loss/train 0.13998794555664062\n",
      "Loss/train 0.1593422293663025\n",
      "Loss/train 0.13677437603473663\n",
      "Loss/train 0.13343101739883423\n",
      "Loss/train 0.1359172761440277\n",
      "Loss/train 0.12976188957691193\n",
      "Loss/train 0.1364571750164032\n",
      "Loss/train 0.16779232025146484\n",
      "Loss/train 0.141912043094635\n",
      "Loss/train 0.1672690212726593\n",
      "Loss/train 0.14905887842178345\n",
      "Loss/train 0.14069139957427979\n",
      "Loss/train 0.13398469984531403\n",
      "Loss/train 0.138578861951828\n",
      "Loss/train 0.15970370173454285\n",
      "Loss/train 0.14541053771972656\n",
      "Loss/train 0.14087560772895813\n",
      "Loss/train 0.13794061541557312\n",
      "Loss/train 0.12738558650016785\n",
      "Loss/train 0.15275025367736816\n",
      "Loss/train 0.12826143205165863\n",
      "Loss/train 0.14356565475463867\n",
      "Loss/train 0.13018196821212769\n",
      "Loss/train 0.14333012700080872\n",
      "Loss/train 0.12830570340156555\n",
      "Loss/train 0.12371661514043808\n",
      "Loss/train 0.1461823582649231\n",
      "Loss/train 0.13192923367023468\n",
      "Loss/train 0.12796896696090698\n",
      "Loss/train 0.12646490335464478\n",
      "Loss/train 0.14216798543930054\n",
      "Loss/train 0.13444018363952637\n",
      "Loss/train 0.13737869262695312\n",
      "Loss/train 0.1513388305902481\n",
      "Loss/train 0.13074520230293274\n",
      "Loss/train 0.14155027270317078\n",
      "Loss/train 0.15344274044036865\n",
      "Loss/train 0.1283206343650818\n",
      "Loss/train 0.15341392159461975\n",
      "Loss/train 0.14289432764053345\n",
      "Loss/train 0.13597722351551056\n",
      "Loss/train 0.12317661941051483\n",
      "Loss/train 0.13555943965911865\n",
      "Loss/train 0.12263014912605286\n",
      "Loss/train 0.1252191960811615\n",
      "Loss/train 0.1281544268131256\n",
      "Loss/train 0.13924220204353333\n",
      "Loss/train 0.12538260221481323\n",
      "Loss/train 0.13363870978355408\n",
      "Loss/train 0.14081066846847534\n",
      "Loss/train 0.13062427937984467\n",
      "Loss/train 0.11864271014928818\n",
      "Loss/train 0.1345200389623642\n",
      "Loss/train 0.12663309276103973\n",
      "Loss/train 0.12482577562332153\n",
      "Loss/train 0.1413557231426239\n",
      "Loss/train 0.12053415924310684\n",
      "Loss/train 0.14668112993240356\n",
      "Loss/train 0.1486155092716217\n",
      "Loss/train 0.12523075938224792\n",
      "Loss/train 0.12171526253223419\n",
      "Loss/train 0.13258220255374908\n",
      "Loss/train 0.12490089237689972\n",
      "Loss/train 0.14020265638828278\n",
      "Loss/train 0.14166435599327087\n",
      "Loss/train 0.17628423869609833\n",
      "Loss/train 0.14773613214492798\n",
      "Loss/train 0.15135586261749268\n",
      "Loss/train 0.15437453985214233\n",
      "Loss/train 0.15283766388893127\n",
      "Loss/train 0.13524900376796722\n",
      "Loss/train 0.12661492824554443\n",
      "Loss/train 0.14842750132083893\n",
      "Loss/train 0.1336974948644638\n",
      "Loss/train 0.14128312468528748\n",
      "Loss/train 0.13001585006713867\n",
      "Loss/train 0.12556660175323486\n",
      "Loss/train 0.13940134644508362\n",
      "Loss/train 0.14229260385036469\n",
      "Loss/train 0.12079278379678726\n",
      "Loss/train 0.14636844396591187\n",
      "Loss/train 0.13627099990844727\n",
      "Loss/train 0.153217613697052\n",
      "Loss/train 0.14298173785209656\n",
      "Loss/train 0.12983651459217072\n",
      "Loss/train 0.1598946452140808\n",
      "Loss/train 0.13207808136940002\n",
      "Loss/train 0.1360715627670288\n",
      "Loss/train 0.1313260793685913\n",
      "Loss/train 0.145208477973938\n",
      "Loss/train 0.1306459903717041\n",
      "Loss/train 0.1328183114528656\n",
      "Loss/train 0.12836042046546936\n",
      "Loss/train 0.12165567278862\n",
      "Loss/train 0.14701154828071594\n",
      "Loss/train 0.13412800431251526\n",
      "Loss/train 0.13503216207027435\n",
      "Loss/train 0.12128805369138718\n",
      "Loss/train 0.14793024957180023\n",
      "Loss/train 0.14629879593849182\n",
      "Loss/train 0.11657574772834778\n",
      "Loss/train 0.11716952174901962\n",
      "Loss/train 0.12714126706123352\n",
      "Loss/train 0.1310378909111023\n",
      "Loss/train 0.12450039386749268\n",
      "Loss/train 0.12741276621818542\n",
      "Loss/train 0.1692851185798645\n",
      "Loss/train 0.11774714291095734\n",
      "Loss/train 0.13014096021652222\n",
      "Loss/train 0.13341708481311798\n",
      "Loss/train 0.11993774771690369\n",
      "Loss/train 0.14687113463878632\n",
      "Loss/train 0.14054127037525177\n",
      "Loss/train 0.12876005470752716\n",
      "Loss/train 0.12031440436840057\n",
      "Loss/train 0.114708811044693\n",
      "Loss/train 0.13366824388504028\n",
      "Loss/train 0.11987976729869843\n",
      "Loss/train 0.1331874430179596\n",
      "Loss/train 0.12672074139118195\n",
      "Loss/train 0.13471007347106934\n",
      "Loss/train 0.12515732645988464\n",
      "Loss/train 0.11988480389118195\n",
      "Loss/train 0.13579532504081726\n",
      "Loss/train 0.12868154048919678\n",
      "Loss/train 0.1443682163953781\n",
      "Loss/train 0.13773305714130402\n",
      "Loss/train 0.14885449409484863\n",
      "Loss/train 0.12825259566307068\n",
      "Loss/train 0.13332536816596985\n",
      "Loss/train 0.11657533049583435\n",
      "Loss/train 0.1230013445019722\n",
      "Loss/train 0.13655710220336914\n",
      "Loss/train 0.12493672966957092\n",
      "Loss/train 0.12873786687850952\n",
      "Loss/train 0.15039053559303284\n",
      "Loss/train 0.14102444052696228\n",
      "Loss/train 0.11417397856712341\n",
      "Loss/train 0.15417122840881348\n",
      "Loss/train 0.1332058310508728\n",
      "Loss/train 0.12667006254196167\n",
      "Loss/train 0.11049863696098328\n",
      "Loss/train 0.12653881311416626\n",
      "Loss/train 0.1331089437007904\n",
      "Loss/train 0.1155097633600235\n",
      "Loss/train 0.13074025511741638\n",
      "Loss/train 0.12534894049167633\n",
      "Loss/train 0.1554497927427292\n",
      "Loss/train 0.12770980596542358\n",
      "Loss/train 0.12207584828138351\n",
      "Loss/train 0.13496266305446625\n",
      "Loss/train 0.11889524757862091\n",
      "Loss/train 0.13066624104976654\n",
      "Loss/train 0.13645678758621216\n",
      "Loss/train 0.11746686697006226\n",
      "Loss/train 0.13590511679649353\n",
      "Loss/train 0.1334749162197113\n",
      "Loss/train 0.12825843691825867\n",
      "Loss/train 0.1335296779870987\n",
      "Loss/train 0.12618720531463623\n",
      "Loss/train 0.14442658424377441\n",
      "Loss/train 0.14368507266044617\n",
      "Loss/train 0.1312369406223297\n",
      "Loss/train 0.11331531405448914\n",
      "Loss/train 0.12341378629207611\n",
      "Loss/train 0.11258436739444733\n",
      "Loss/train 0.12304829806089401\n",
      "Loss/train 0.1282396763563156\n",
      "Loss/train 0.13349661231040955\n",
      "Loss/train 0.13500171899795532\n",
      "Loss/train 0.12005642801523209\n",
      "Loss/train 0.1391967087984085\n",
      "Loss/train 0.1104542538523674\n",
      "Loss/train 0.12944301962852478\n",
      "Loss/train 0.125282421708107\n",
      "Loss/train 0.16051533818244934\n",
      "Loss/train 0.13851696252822876\n",
      "Loss/train 0.12976689636707306\n",
      "Loss/train 0.1314745396375656\n",
      "Loss/train 0.1227063238620758\n",
      "Loss/train 0.1377764493227005\n",
      "Loss/train 0.1627269685268402\n",
      "Loss/train 0.10615791380405426\n",
      "Loss/train 0.11986662447452545\n",
      "Loss/train 0.11749623715877533\n",
      "Loss/train 0.12308132648468018\n",
      "Loss/train 0.12227232754230499\n",
      "Loss/train 0.12046252936124802\n",
      "Loss/train 0.1267726570367813\n",
      "Loss/train 0.12348794937133789\n",
      "Loss/train 0.11457299441099167\n",
      "Loss/train 0.11396118998527527\n",
      "Loss/train 0.1241573691368103\n",
      "Loss/train 0.12614405155181885\n",
      "Loss/train 0.10411840677261353\n",
      "Loss/train 0.11785497516393661\n",
      "Loss/train 0.11285141110420227\n",
      "Loss/train 0.13768526911735535\n",
      "Loss/train 0.12240610271692276\n",
      "Loss/train 0.11517681181430817\n",
      "Loss/train 0.13081811368465424\n",
      "Loss/train 0.1261393427848816\n",
      "Loss/train 0.11777929216623306\n",
      "Loss/train 0.14408838748931885\n",
      "Loss/train 0.11430720239877701\n",
      "Loss/train 0.12838858366012573\n",
      "Loss/train 0.12427820265293121\n",
      "Loss/train 0.1527542769908905\n",
      "Loss/train 0.10710613429546356\n",
      "Loss/train 0.10454970598220825\n",
      "Loss/train 0.1340399533510208\n",
      "Loss/train 0.10758105665445328\n",
      "Loss/train 0.1424694061279297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss/train 0.11575613170862198\n",
      "Loss/train 0.13267311453819275\n",
      "Loss/train 0.10915237665176392\n",
      "Loss/train 0.1358335018157959\n",
      "Loss/train 0.11258839815855026\n",
      "Loss/train 0.11967572569847107\n",
      "Loss/train 0.10483244806528091\n",
      "Loss/train 0.11667843163013458\n",
      "Loss/train 0.12309788167476654\n",
      "Loss/train 0.10093630850315094\n",
      "Loss/train 0.12057138979434967\n",
      "Loss/train 0.12130872905254364\n",
      "Loss/train 0.10840428620576859\n",
      "Loss/train 0.14752553403377533\n",
      "Loss/train 0.10888612270355225\n",
      "Loss/train 0.10901141166687012\n",
      "Loss/train 0.10479117184877396\n",
      "Loss/train 0.11764159798622131\n",
      "Loss/train 0.10858500003814697\n",
      "Loss/train 0.127885103225708\n",
      "Loss/train 0.10381962358951569\n",
      "Loss/train 0.11429998278617859\n",
      "Loss/train 0.10490847378969193\n",
      "Loss/train 0.11155711859464645\n",
      "Loss/train 0.12720413506031036\n",
      "Loss/train 0.11900614947080612\n",
      "Loss/train 0.12517857551574707\n",
      "Loss/train 0.10609009861946106\n",
      "Loss/train 0.10340137779712677\n",
      "Loss/train 0.12136414647102356\n",
      "Loss/train 0.12691758573055267\n",
      "Loss/train 0.11697518825531006\n",
      "Loss/train 0.11100713163614273\n",
      "Loss/train 0.11382290720939636\n",
      "Loss/train 0.11597608029842377\n",
      "Loss/train 0.12543100118637085\n",
      "Loss/train 0.12911216914653778\n",
      "Loss/train 0.1274528205394745\n",
      "Loss/train 0.09892243146896362\n",
      "Loss/train 0.10099473595619202\n",
      "Loss/train 0.10367384552955627\n",
      "Loss/train 0.12524087727069855\n",
      "Loss/train 0.11086556315422058\n",
      "Loss/train 0.11701877415180206\n",
      "Loss/train 0.10574771463871002\n",
      "Loss/train 0.1162802204489708\n",
      "Loss/train 0.14045843482017517\n",
      "Loss/train 0.10924451053142548\n",
      "Loss/train 0.11639824509620667\n",
      "Loss/train 0.11503893882036209\n",
      "Loss/train 0.10789547860622406\n",
      "Loss/train 0.11613504588603973\n",
      "Loss/train 0.11184509843587875\n",
      "Loss/train 0.11624357104301453\n",
      "Loss/train 0.14165571331977844\n",
      "Loss/train 0.10032357275485992\n",
      "Loss/train 0.10044889152050018\n",
      "Loss/train 0.11934495717287064\n",
      "Loss/train 0.1064579039812088\n",
      "Loss/train 0.11154014617204666\n",
      "Loss/train 0.11685146391391754\n",
      "Loss/train 0.12294773012399673\n",
      "Loss/train 0.10564549267292023\n",
      "Loss/train 0.10226511210203171\n",
      "Loss/train 0.09921051561832428\n",
      "Loss/train 0.11971129477024078\n",
      "Loss/train 0.13336136937141418\n",
      "Loss/train 0.10782875120639801\n",
      "Loss/train 0.12661229074001312\n",
      "Loss/train 0.121306411921978\n",
      "Loss/train 0.12697026133537292\n",
      "Loss/train 0.10450154542922974\n",
      "Loss/train 0.11767130345106125\n",
      "Loss/train 0.11523410677909851\n",
      "Loss/train 0.11845608055591583\n",
      "Loss/train 0.11205004155635834\n",
      "Loss/train 0.10930632054805756\n",
      "Loss/train 0.11348928511142731\n",
      "Loss/train 0.10564740002155304\n",
      "Loss/train 0.09742134064435959\n",
      "Loss/train 0.10440181195735931\n",
      "Loss/train 0.11934195458889008\n",
      "Loss/train 0.10422226041555405\n",
      "Loss/train 0.1221170425415039\n",
      "Loss/train 0.11621604859828949\n",
      "Loss/train 0.10243894904851913\n",
      "Loss/train 0.12396177649497986\n",
      "Loss/train 0.09653058648109436\n",
      "Loss/train 0.09850233793258667\n",
      "Loss/train 0.10155623406171799\n",
      "Loss/train 0.10553434491157532\n",
      "Loss/train 0.09954223036766052\n",
      "Loss/train 0.11129473149776459\n",
      "Loss/train 0.1018117293715477\n",
      "Loss/train 0.11860021203756332\n",
      "Loss/train 0.11569100618362427\n",
      "Loss/train 0.10817573964595795\n",
      "Loss/train 0.11058026552200317\n",
      "Loss/train 0.10007193684577942\n",
      "Loss/train 0.10590942949056625\n",
      "Loss/train 0.11530901491641998\n",
      "Loss/train 0.11164125800132751\n",
      "Loss/train 0.09179557859897614\n",
      "Loss/train 0.12281621992588043\n",
      "Loss/train 0.10803821682929993\n",
      "Loss/train 0.10180412232875824\n",
      "Loss/train 0.10596956312656403\n",
      "Loss/train 0.12171195447444916\n",
      "Loss/train 0.10822054743766785\n",
      "Loss/train 0.11448734253644943\n",
      "Loss/train 0.09131582081317902\n",
      "Loss/train 0.10590574145317078\n",
      "Loss/train 0.10123610496520996\n",
      "Loss/train 0.09636015444993973\n",
      "Loss/train 0.0995173305273056\n",
      "Loss/train 0.11159413307905197\n",
      "Loss/train 0.1114012598991394\n",
      "Loss/train 0.10991600155830383\n",
      "Loss/train 0.08979905396699905\n",
      "Loss/train 0.09708422422409058\n",
      "Loss/train 0.11152520030736923\n",
      "Loss/train 0.09407810866832733\n",
      "Loss/train 0.10423989593982697\n",
      "Loss/train 0.10992877930402756\n",
      "Loss/train 0.10483656078577042\n",
      "Loss/train 0.09489350765943527\n",
      "Loss/train 0.11885276436805725\n",
      "Loss/train 0.09129008650779724\n"
     ]
    }
   ],
   "source": [
    "from utils.dataset import ISBI_Loader\n",
    "from torch import optim\n",
    "\n",
    "def train_net(net, device, data_path, epochs=40, batch_size=1, lr=0.00001):\n",
    "    # \n",
    "    isbi_dataset = ISBI_Loader(data_path)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=isbi_dataset,\n",
    "                                               batch_size=batch_size, \n",
    "                                               shuffle=True)\n",
    "    # RMSprop\n",
    "    optimizer = optim.RMSprop(net.parameters(), lr=lr, weight_decay=1e-8, momentum=0.9)\n",
    "    # Loss\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    # best_loss\n",
    "    best_loss = float('inf')\n",
    "    # epochs\n",
    "    for epoch in range(epochs):\n",
    "        # \n",
    "        net.train()\n",
    "        # batch_size\n",
    "        for image, label in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            # device\n",
    "            image = image.to(device=device, dtype=torch.float32)\n",
    "            label = label.to(device=device, dtype=torch.float32)\n",
    "            # \n",
    "            pred = net(image)\n",
    "            # loss\n",
    "            loss = criterion(pred, label)\n",
    "            print('Loss/train', loss.item())\n",
    "            # loss\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                torch.save(net.state_dict(), 'best_model.pth')\n",
    "            # \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if epoch = epochs-1:\n",
    "            print(done)\n",
    "\n",
    "# cudacudacpu\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# 11\n",
    "net = UNet(n_channels=1, n_classes=1)\n",
    "# deivce\n",
    "net.to(device=device)\n",
    "# \n",
    "data_path = \"data/train/\"\n",
    "train_net(net, device, data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geyu\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:58: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "C:\\Users\\geyu\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:59: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # cudacudacpu\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # 1\n",
    "    net = UNet(n_channels=1, n_classes=1)\n",
    "    # deivce\n",
    "    net.to(device=device)\n",
    "    # \n",
    "    net.load_state_dict(torch.load('best_model.pth', map_location=device))\n",
    "    # \n",
    "    net.eval()\n",
    "    # \n",
    "    tests_path = glob.glob('data/test/*.png')\n",
    "    # \n",
    "    for test_path in tests_path:\n",
    "        # \n",
    "        save_res_path = test_path.split('.')[0] + '_Seg.png'\n",
    "        # \n",
    "        img = cv2.imread(test_path)\n",
    "        # \n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        # batch11512*512\n",
    "        img = img.reshape(1, 1, img.shape[0], img.shape[1])\n",
    "        # tensor\n",
    "        img_tensor = torch.from_numpy(img)\n",
    "        # tensordevicecpucpucudacuda\n",
    "        img_tensor = img_tensor.to(device=device, dtype=torch.float32)\n",
    "        # \n",
    "        pred = net(img_tensor)\n",
    "        # \n",
    "        pred = np.array(pred.data.cpu()[0])[0]\n",
    "        # \n",
    "        pred[pred >= 0.5] = 255\n",
    "        pred[pred < 0.5] = 0\n",
    "        # \n",
    "        cv2.imwrite(save_res_path, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td><img src='data/test/0.png'></td><td><img src='data/test/0_Seg.png'></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation resuts on DICE, JC, HD, ASD:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8956019996081678, 0.8109413447782546, 129.0, 46.49052762124423)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from pylab import *\n",
    "from medpy import metric\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "display(HTML(\"<table><tr><td><img src='data/test/0.png'></td><td><img src='data/test/0_Seg.png'></td></tr></table>\"))\n",
    "\n",
    "def calculate_metric_percase(pred, gt):\n",
    "    \n",
    "    dice = metric.binary.dc(pred, gt)\n",
    "    jc = metric.binary.jc(pred, gt)\n",
    "    hd = metric.binary.hd95(pred, gt)\n",
    "    asd = metric.binary.asd(pred, gt)\n",
    "    return dice, jc, hd, asd\n",
    "\n",
    "#\n",
    "im_gt = array(Image.open(\"data/test/0.png\"))\n",
    "im_pred = array(Image.open(\"data/test/0_Seg.png\"))\n",
    "\n",
    "print('Evaluation resuts on DICE, JC, HD, ASD:')\n",
    "calculate_metric_percase(im_pred, im_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
